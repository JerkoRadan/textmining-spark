{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all required libraries here. There are some stuff that I had used previously but have not removed here\n",
    "from pyspark import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import os\n",
    "from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType\n",
    "#importing pyspark machine learning libraries\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have a 8 gb ram so I have 7.6 gb available to use so I am allowing max java heap to use 6 gb\n",
    "from pyspark import SparkConf\n",
    "conf=SparkConf()\n",
    "conf.set(\"spark.driver.memory\", \"3g\")\n",
    "conf.set(\"spark.executor.memory\", \"3g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't run this. This is the alternative if driver keeps running out of memory\n",
    "#Executor memory can be ignored while using spark local.\n",
    "#from pyspark import SparkConf\n",
    "#conf=SparkConf()\n",
    "#conf.set(\"spark.driver.memory\", \"7g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining spark context and spark session\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the schema of the json texts\n",
    "schema = StructType([\n",
    "    StructField('title_page', StringType(), True),\n",
    "    StructField('text_new', StringType(), True),\n",
    "    StructField('text_old', StringType(), True),\n",
    "    StructField('name_user', StringType(), True),\n",
    "    StructField('label', StringType(), True),\n",
    "    StructField('comment', StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this would recursively read all json files into the same data frame. ** wildcard refers to all directories\n",
    "#This is the easiest way to read json in spark\n",
    "df_json = spark.read.json(\"/home/shourya/fold/**/part-*\",schema, multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this would print the schema of the pyspark sql dataframe\n",
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is how we can convert it to a pandas dataframe which is easier to work with or for checking stuff\n",
    "pandas = df_json.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting unique tokens in labels\n",
    "from pyspark.sql.functions import col\n",
    "df_json.groupBy(\"label\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting unique tokens of user names\n",
    "df_json.groupBy(\"name_user\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't run this\n",
    "#calculate the difference between two text columns using spark\n",
    "#Output is going to be a list and it would skip the nulls so dimensions would differ from actual df dimension\n",
    "#collect = df_json.select('text_new').subtract(df_json.select('text_old')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't run this\n",
    "#This would print the frequency of words in the difference. It can show which are the most frequently changed words.\n",
    "#wordfreq = []\n",
    "#for w in collect:\n",
    "#    wordfreq.append(collect.count(w))\n",
    "\n",
    "#print(\"Pairs\\n\" + str(list(zip(collect, wordfreq))))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't run this\n",
    "# convert the difference to a pandas dataframe with the charctare \"a\" as column name.\n",
    "#Typecasting the list to a dataframe for ease of use.\n",
    "#import numpy as np\n",
    "#len(collect)\n",
    "#df_collect = pd.DataFrame(np.array(collect).reshape(225,1), columns = list(\"a\"))\n",
    "#df_collect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "# regular expression tokenizer\n",
    "regexTokenizer_new = RegexTokenizer(inputCol=\"text_new\", outputCol=\"words_new\", pattern=\"\\\\W\")\n",
    "regexTokenizer_old = RegexTokenizer(inputCol=\"text_old\", outputCol=\"words_old\", pattern=\"\\\\W\")\n",
    "regexTokenizer_title = RegexTokenizer(inputCol=\"title_page\", outputCol=\"words_title_page\", pattern=\"\\\\W\")\n",
    "regexTokenizer_user = RegexTokenizer(inputCol=\"name_user\", outputCol=\"words_name_user\", pattern=\"\\\\W\")\n",
    "regexTokenizer_comment = RegexTokenizer(inputCol=\"comment\", outputCol=\"words_comment\", pattern=\"\\\\W\")\n",
    "# stop words\n",
    "add_stopwords = [\"a\",\"an\",\"the\",\"on\",]\n",
    "stopwordsRemover_new = StopWordsRemover(inputCol=\"words_new\", outputCol=\"filtered_new\").setStopWords(add_stopwords)\n",
    "stopwordsRemover_old = StopWordsRemover(inputCol=\"words_old\", outputCol=\"filtered_old\").setStopWords(add_stopwords)\n",
    "stopwordsRemover_comment = StopWordsRemover(inputCol=\"words_comment\", outputCol=\"filtered_comment\").setStopWords(add_stopwords)\n",
    "# bag of words count\n",
    "countVectors_new = CountVectorizer(inputCol=\"filtered_new\", outputCol=\"features_new\", vocabSize=100000, minDF=5)\n",
    "countVectors_old = CountVectorizer(inputCol=\"filtered_old\", outputCol=\"features_old\", vocabSize=100000, minDF=5)\n",
    "countVectors_comment = CountVectorizer(inputCol=\"filtered_comment\", outputCol=\"features_comment\", vocabSize=100000, minDF=5)\n",
    "countVectors_user = CountVectorizer(inputCol=\"words_name_user\", outputCol=\"features_name_user\", vocabSize=100000, minDF=5)\n",
    "countVectors_title = CountVectorizer(inputCol=\"words_title_page\", outputCol=\"features_title_page\", vocabSize=100000, minDF=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the name of the column 'label' to 'category' as pyspark logistic model does not accept anything but 'label' as target.\n",
    "df_json = df_json.withColumnRenamed(\"label\", \"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "#This would encode label into numerical values\n",
    "label_stringIdx = StringIndexer(inputCol = \"category\", outputCol = \"label\")\n",
    "\n",
    "#Pipeline for creating jobs by spark. This does not have a logical consequence in our case but essential.\n",
    "pipeline = Pipeline(stages=[label_stringIdx, regexTokenizer_new, regexTokenizer_old, regexTokenizer_title, regexTokenizer_user, regexTokenizer_comment, stopwordsRemover_new, stopwordsRemover_old, stopwordsRemover_comment, countVectors_comment, countVectors_new, countVectors_old, countVectors_user, countVectors_title])\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(df_json)\n",
    "dataset = pipelineFit.transform(df_json)\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "#Training and test split\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression model definition. This is a very naive attempt and would probably overfit without regularization.\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "#Pyspark would have us assemble the features using vector assembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['features_new','features_old','features_comment','features_name_user','features_title_page'],\n",
    "    outputCol='features')\n",
    "#Transforming the training data using the assembler\n",
    "assembler.transform(trainingData)\n",
    "#Pipeline based execution which is charcteristic for spark.\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "#Fitting the model\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the model performance on testData\n",
    "predictions = model.transform(testData)\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "#TF IDF based transformation in spark\n",
    "hashingTF_name = HashingTF(inputCol=\"words_name_user\", outputCol=\"rawFeatures_name\", numFeatures=100000)\n",
    "hashingTF_title = HashingTF(inputCol=\"words_title_page\", outputCol=\"rawFeatures_title\", numFeatures=100000)\n",
    "hashingTF_new = HashingTF(inputCol=\"filtered_new\", outputCol=\"rawFeatures_new\", numFeatures=100000)\n",
    "hashingTF_old = HashingTF(inputCol=\"filtered_old\", outputCol=\"rawFeatures_old\", numFeatures=100000)\n",
    "hashingTF_comment = HashingTF(inputCol=\"filtered_comment\", outputCol=\"rawFeatures_comment\", numFeatures=100000)\n",
    "idf_name = IDF(inputCol=\"rawFeatures_name\", outputCol=\"features_idf_name\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "idf_title = IDF(inputCol=\"rawFeatures_title\", outputCol=\"features_idf_title\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "idf_new = IDF(inputCol=\"rawFeatures_new\", outputCol=\"features_idf_new\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "idf_old = IDF(inputCol=\"rawFeatures_old\", outputCol=\"features_idf_old\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "idf_comment = IDF(inputCol=\"rawFeatures_comment\", outputCol=\"features_idf_comment\", minDocFreq=5) #minDocFreq: remove sparse terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same style of execution as the first logistic model\n",
    "pipeline_ti = Pipeline(stages=[label_stringIdx, regexTokenizer_new, regexTokenizer_old, regexTokenizer_title, regexTokenizer_user, regexTokenizer_comment, stopwordsRemover_new, stopwordsRemover_old, stopwordsRemover_comment, countVectors_comment, countVectors_new, countVectors_old, countVectors_user, countVectors_title, hashingTF_name, hashingTF_title, hashingTF_new, hashingTF_old, hashingTF_comment, idf_name, idf_title, idf_new, idf_old, idf_comment])\n",
    "pipelineFit_ti = pipeline_ti.fit(df_json)\n",
    "dataset_ti = pipelineFit_ti.transform(df_json)\n",
    "(trainingData_ti, testData_ti) = dataset_ti.randomSplit([0.7, 0.3], seed = 100)\n",
    "lr_ti = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "assembler_ti = VectorAssembler(\n",
    "    inputCols=['rawFeatures_name','rawFeatures_title','rawFeatures_new','rawFeatures_old','rawFeatures_comment','features_idf_name','features_idf_title','features_idf_new','features_idf_old','features_idf_comment'],\n",
    "    outputCol='features')\n",
    "assembler_ti.transform(trainingData_ti)\n",
    "pipeline_ti = Pipeline(stages=[assembler_ti, lr_ti])\n",
    "trainingData_ti.cache()\n",
    "testData_ti.cache()\n",
    "model_ti = pipeline_ti.fit(trainingData_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "#In this section we try to run the previous model with various parameter values\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr_ti.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr_ti.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "#            .addGrid(model_ti.maxIter, [10, 20, 50]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build())\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr_ti, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator_ti, \\\n",
    "                    numFolds=5)\n",
    "assembler_ti_cv = VectorAssembler(\n",
    "    inputCols=['rawFeatures_name','rawFeatures_title','rawFeatures_new','rawFeatures_old','rawFeatures_comment','features_idf_name','features_idf_title','features_idf_new','features_idf_old','features_idf_comment'],\n",
    "    outputCol='features')\n",
    "assembler_ti_cv.transform(trainingData_ti)\n",
    "pipeline_ti_cv = Pipeline(stages=[assembler_ti_cv, cv])\n",
    "trainingData_ti.cache()\n",
    "testData_ti.cache()\n",
    "model_ti_cv = pipeline_ti_cv.fit(trainingData_ti)\n",
    "\n",
    "predictions = model_ti_cv.transform(testData_ti)\n",
    "# Evaluate best model\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
