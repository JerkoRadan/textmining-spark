{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all required libraries here. There are some stuff that I had used previously but have not removed here\n",
    "from pyspark import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import os\n",
    "from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType\n",
    "#importing pyspark machine learning libraries\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.104:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining spark context and spark session\n",
    "spark = SparkSession(sc)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the schema of the json texts\n",
    "schema = StructType([\n",
    "    StructField('title_page', StringType(), True),\n",
    "    StructField('text_new', StringType(), True),\n",
    "    StructField('text_old', StringType(), True),\n",
    "    StructField('name_user', StringType(), True),\n",
    "    StructField('label', StringType(), True),\n",
    "    StructField('comment', StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this would recursively read all json files into the same data frame. ** wildcard refers to all directories\n",
    "#This is the easiest way to read json in spark\n",
    "df_json = spark.read.json(\"c:/spark/saves/**/part-*\",schema, multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title_page: string (nullable = true)\n",
      " |-- text_new: string (nullable = true)\n",
      " |-- text_old: string (nullable = true)\n",
      " |-- name_user: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#this would print the schema of the pyspark sql dataframe\n",
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    title_page  \\\n",
      "0                               Chris Sparling   \n",
      "1                         Genocides in history   \n",
      "2     2016 United States presidential election   \n",
      "3                          List of iOS devices   \n",
      "4                          List of iOS devices   \n",
      "...                                        ...   \n",
      "3960                         George Conway III   \n",
      "3961                          George T. Conway   \n",
      "3962                                  Sambavar   \n",
      "3963                                  Sambavar   \n",
      "3964                                  Sambavar   \n",
      "\n",
      "                                               text_new  \\\n",
      "0     {{Infobox person\\n| name                      ...   \n",
      "1     {{Use American English|date=August 2015}}\\n{{U...   \n",
      "2     {{For|related races|2016 United States electio...   \n",
      "3     {{short description|Wikipedia list article}}\\n...   \n",
      "4     {{short description|Wikipedia list article}}\\n...   \n",
      "...                                                 ...   \n",
      "3960  #redirect[[George T. Conway III]]{{R from shor...   \n",
      "3961  #REDIRECT[[George T. Conway III]]{{R from shor...   \n",
      "3962                               [[Tamil Sambavar]]\\n   \n",
      "3963                               [[Tamil Sambavar]]\\n   \n",
      "3964                               [[Tamil Sambavar]]\\n   \n",
      "\n",
      "                                               text_old   name_user   label  \\\n",
      "0     {{Infobox person\\n| name                      ...      Lrapsc    safe   \n",
      "1     {{Use American English|date=August 2015}}\\n{{U...       OAO24  unsafe   \n",
      "2     {{For|related races|2016 United States electio...     Gggiann    safe   \n",
      "3     {{short description|Wikipedia list article}}\\n...    Quebec99    safe   \n",
      "4     {{short description|Wikipedia list article}}\\n...    Quebec99    safe   \n",
      "...                                                 ...         ...     ...   \n",
      "3960                #redirect[[George T. Conway III]]\\n     Bagumba    safe   \n",
      "3961                #REDIRECT[[George T. Conway III]]\\n     Bagumba    safe   \n",
      "3962                                   [[Adi Andhra]]\\n  Wiseinfo07    safe   \n",
      "3963                                   [[Adi Andhra]]\\n  Wiseinfo07    safe   \n",
      "3964                                   [[Adi Andhra]]\\n  Wiseinfo07    safe   \n",
      "\n",
      "                                                comment  \n",
      "0     Made slight adjustment to release date for \"Th...  \n",
      "1                                                        \n",
      "2        →‎Notable expressions, phrases, and statements  \n",
      "3                Fix duplicate ref names –You can help!  \n",
      "4                Fix duplicate ref names –You can help!  \n",
      "...                                                 ...  \n",
      "3960                                               +cat  \n",
      "3961                                               +cat  \n",
      "3962                                                     \n",
      "3963                                                     \n",
      "3964                                                     \n",
      "\n",
      "[3965 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#this is how we can convert it to a pandas dataframe which is easier to work with or for checking stuff\n",
    "pandas = df_json.select(\"*\").toPandas()\n",
    "print(pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| label|count|\n",
      "+------+-----+\n",
      "|  safe| 3397|\n",
      "|unsafe|  544|\n",
      "|vandal|   24|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Counting unique tokens in labels\n",
    "from pyspark.sql.functions import col\n",
    "df_json.groupBy(\"label\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           name_user|count|\n",
      "+--------------------+-----+\n",
      "|                Rodw|  277|\n",
      "|        Red Director|  170|\n",
      "|             Arjayay|  120|\n",
      "|             Majavah|   82|\n",
      "|               Ss112|   71|\n",
      "|             Wbm1058|   49|\n",
      "|            Vanisaac|   45|\n",
      "|             Moe1810|   41|\n",
      "|              GünniX|   38|\n",
      "|Ser Amantio di Ni...|   33|\n",
      "|   Chris the speller|   30|\n",
      "|       Bellowhead678|   29|\n",
      "|            Rjwilmsi|   28|\n",
      "|          Epicgenius|   26|\n",
      "|                Zyxw|   24|\n",
      "|              Menchi|   23|\n",
      "|             Lugnuts|   20|\n",
      "|         Robby.is.on|   20|\n",
      "|            Jprg1966|   19|\n",
      "|            Nemo bis|   18|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#counting unique tokens of user names\n",
    "df_json.groupBy(\"name_user\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't run this\n",
    "#calculate the difference between two text columns using spark\n",
    "#Output is going to be a list and it would skip the nulls so dimensions would differ from actual df dimension\n",
    "#collect = df_json.select('text_new').subtract(df_json.select('text_old')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't run this\n",
    "#This would print the frequency of words in the difference. It can show which are the most frequently changed words.\n",
    "#wordfreq = []\n",
    "#for w in collect:\n",
    "#    wordfreq.append(collect.count(w))\n",
    "\n",
    "#print(\"Pairs\\n\" + str(list(zip(collect, wordfreq))))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't run this\n",
    "# convert the difference to a pandas dataframe with the charctare \"a\" as column name.\n",
    "#Typecasting the list to a dataframe for ease of use.\n",
    "#import numpy as np\n",
    "#len(collect)\n",
    "#df_collect = pd.DataFrame(np.array(collect).reshape(225,1), columns = list(\"a\"))\n",
    "#df_collect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "# regular expression tokenizer\n",
    "regexTokenizer_new = RegexTokenizer(inputCol=\"text_new\", outputCol=\"words_new\", pattern=\"\\\\W\")\n",
    "regexTokenizer_old = RegexTokenizer(inputCol=\"text_old\", outputCol=\"words_old\", pattern=\"\\\\W\")\n",
    "regexTokenizer_title = RegexTokenizer(inputCol=\"title_page\", outputCol=\"words_title_page\", pattern=\"\\\\W\")\n",
    "regexTokenizer_user = RegexTokenizer(inputCol=\"name_user\", outputCol=\"words_name_user\", pattern=\"\\\\W\")\n",
    "regexTokenizer_comment = RegexTokenizer(inputCol=\"comment\", outputCol=\"words_comment\", pattern=\"\\\\W\")\n",
    "# stop words\n",
    "add_stopwords = [\"a\",\"an\",\"the\",\"on\",]\n",
    "stopwordsRemover_new = StopWordsRemover(inputCol=\"words_new\", outputCol=\"filtered_new\").setStopWords(add_stopwords)\n",
    "stopwordsRemover_old = StopWordsRemover(inputCol=\"words_old\", outputCol=\"filtered_old\").setStopWords(add_stopwords)\n",
    "stopwordsRemover_comment = StopWordsRemover(inputCol=\"words_comment\", outputCol=\"filtered_comment\").setStopWords(add_stopwords)\n",
    "# bag of words count\n",
    "countVectors_new = CountVectorizer(inputCol=\"filtered_new\", outputCol=\"features_new\", vocabSize=100000, minDF=5)\n",
    "countVectors_old = CountVectorizer(inputCol=\"filtered_old\", outputCol=\"features_old\", vocabSize=100000, minDF=5)\n",
    "countVectors_comment = CountVectorizer(inputCol=\"filtered_comment\", outputCol=\"features_comment\", vocabSize=100000, minDF=5)\n",
    "countVectors_user = CountVectorizer(inputCol=\"words_name_user\", outputCol=\"features_name_user\", vocabSize=100000, minDF=5)\n",
    "countVectors_title = CountVectorizer(inputCol=\"words_title_page\", outputCol=\"features_title_page\", vocabSize=100000, minDF=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the name of the column 'label' to 'category' as pyspark logistic model does not accept anything but 'label' as target.\n",
    "df_json = df_json.withColumnRenamed(\"label\", \"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---------+--------+--------------------+-----+--------------------+--------------------+--------------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+\n",
      "|          title_page|            text_new|            text_old|name_user|category|             comment|label|           words_new|           words_old|    words_title_page|words_name_user|       words_comment|        filtered_new|        filtered_old|    filtered_comment|    features_comment|        features_new|        features_old|features_name_user| features_title_page|\n",
      "+--------------------+--------------------+--------------------+---------+--------+--------------------+-----+--------------------+--------------------+--------------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+\n",
      "|      Chris Sparling|{{Infobox person\n",
      "...|{{Infobox person\n",
      "...|   Lrapsc|    safe|Made slight adjus...|  0.0|[infobox, person,...|[infobox, person,...|   [chris, sparling]|       [lrapsc]|[made, slight, ad...|[infobox, person,...|[infobox, person,...|[made, slight, ad...|(465,[10,13,39,54...|(62991,[0,1,2,3,4...|(62880,[0,1,2,3,4...|       (265,[],[])|   (358,[176],[1.0])|\n",
      "|Genocides in history|{{Use American En...|{{Use American En...|    OAO24|  unsafe|                    |  1.0|[use, american, e...|[use, american, e...|[genocides, in, h...|        [oao24]|                  []|[use, american, e...|[use, american, e...|                  []|         (465,[],[])|(62991,[0,1,2,3,4...|(62880,[0,1,2,3,4...|       (265,[],[])|(358,[2,64],[1.0,...|\n",
      "|2016 United State...|{{For|related rac...|{{For|related rac...|  Gggiann|    safe|→‎Notable express...|  0.0|[for, related, ra...|[for, related, ra...|[2016, united, st...|      [gggiann]|[notable, express...|[for, related, ra...|[for, related, ra...|[notable, express...|(465,[10,168],[1....|(62991,[0,1,2,3,4...|(62880,[0,1,2,3,4...|       (265,[],[])|(358,[15,19,101,1...|\n",
      "| List of iOS devices|{{short descripti...|{{short descripti...| Quebec99|    safe|Fix duplicate ref...|  0.0|[short, descripti...|[short, descripti...|[list, of, ios, d...|     [quebec99]|[fix, duplicate, ...|[short, descripti...|[short, descripti...|[fix, duplicate, ...|(465,[32,33,42,53...|(62991,[0,1,2,3,4...|(62880,[0,1,2,3,4...|  (265,[36],[1.0])|(358,[0,3],[1.0,1...|\n",
      "| List of iOS devices|{{short descripti...|{{short descripti...| Quebec99|    safe|Fix duplicate ref...|  0.0|[short, descripti...|[short, descripti...|[list, of, ios, d...|     [quebec99]|[fix, duplicate, ...|[short, descripti...|[short, descripti...|[fix, duplicate, ...|(465,[32,33,42,53...|(62991,[0,1,2,3,4...|(62880,[0,1,2,3,4...|  (265,[36],[1.0])|(358,[0,3],[1.0,1...|\n",
      "+--------------------+--------------------+--------------------+---------+--------+--------------------+-----+--------------------+--------------------+--------------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "#This would encode label into numerical values\n",
    "label_stringIdx = StringIndexer(inputCol = \"category\", outputCol = \"label\")\n",
    "\n",
    "#Pipeline for creating jobs by spark. This does not have a logical consequence in our case but essential.\n",
    "pipeline = Pipeline(stages=[label_stringIdx, regexTokenizer_new, regexTokenizer_old, regexTokenizer_title, regexTokenizer_user, regexTokenizer_comment, stopwordsRemover_new, stopwordsRemover_old, stopwordsRemover_comment, countVectors_comment, countVectors_new, countVectors_old, countVectors_user, countVectors_title])\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(df_json)\n",
    "dataset = pipelineFit.transform(df_json)\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 2756\n",
      "Test Dataset Count: 1209\n"
     ]
    }
   ],
   "source": [
    "# set seed for reproducibility\n",
    "#Training and test split\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression model definition. This is a very naive attempt and would probably overfit without regularization.\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "#Pyspark would have us assemble the features using vector assembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['features_new','features_old','features_comment','features_name_user','features_title_page'],\n",
    "    outputCol='features')\n",
    "#Transforming the training data using the assembler\n",
    "assembler.transform(trainingData)\n",
    "#Pipeline based execution which is charcteristic for spark.\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "#Fitting the model\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the model performance on testData\n",
    "predictions = model.transform(testData)\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "#TF IDF based transformation in spark\n",
    "hashingTF_name = HashingTF(inputCol=\"words_name_user\", outputCol=\"rawFeatures_name\", numFeatures=100000)\n",
    "hashingTF_title = HashingTF(inputCol=\"words_title_page\", outputCol=\"rawFeatures_title\", numFeatures=100000)\n",
    "hashingTF_new = HashingTF(inputCol=\"filtered_new\", outputCol=\"rawFeatures_new\", numFeatures=100000)\n",
    "hashingTF_old = HashingTF(inputCol=\"filtered_old\", outputCol=\"rawFeatures_old\", numFeatures=100000)\n",
    "hashingTF_comment = HashingTF(inputCol=\"filtered_comment\", outputCol=\"rawFeatures_comment\", numFeatures=100000)\n",
    "idf_name = IDF(inputCol=\"rawFeatures_name\", outputCol=\"features_idf_name\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "idf_title = IDF(inputCol=\"rawFeatures_title\", outputCol=\"features_idf_title\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "idf_new = IDF(inputCol=\"rawFeatures_new\", outputCol=\"features_idf_new\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "idf_old = IDF(inputCol=\"rawFeatures_old\", outputCol=\"features_idf_old\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "idf_comment = IDF(inputCol=\"rawFeatures_comment\", outputCol=\"features_idf_comment\", minDocFreq=5) #minDocFreq: remove sparse terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same style of execution as the first logistic model\n",
    "pipeline_ti = Pipeline(stages=[label_stringIdx, regexTokenizer_new, regexTokenizer_old, regexTokenizer_title, regexTokenizer_user, regexTokenizer_comment, stopwordsRemover_new, stopwordsRemover_old, stopwordsRemover_comment, countVectors_comment, countVectors_new, countVectors_old, countVectors_user, countVectors_title, hashingTF_name, hashingTF_title, hashingTF_new, hashingTF_old, hashingTF_comment, idf_name, idf_title, idf_new, idf_old, idf_comment])\n",
    "pipelineFit_ti = pipeline_ti.fit(df_json)\n",
    "dataset_ti = pipelineFit_ti.transform(df_json)\n",
    "(trainingData_ti, testData_ti) = dataset_ti.randomSplit([0.7, 0.3], seed = 100)\n",
    "lr_ti = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "assembler_ti = VectorAssembler(\n",
    "    inputCols=['rawFeatures_name','rawFeatures_title','rawFeatures_new','rawFeatures_old','rawFeatures_comment','features_idf_name','features_idf_title','features_idf_new','features_idf_old','features_idf_comment'],\n",
    "    outputCol='features')\n",
    "assembler_ti.transform(trainingData_ti)\n",
    "pipeline_ti = Pipeline(stages=[assembler_ti, lr_ti])\n",
    "trainingData_ti.cache()\n",
    "testData_ti.cache()\n",
    "model_ti = pipeline_ti.fit(trainingData_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "#In this section we try to run the previous model with various parameter values\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr_ti.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr_ti.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "#            .addGrid(model_ti.maxIter, [10, 20, 50]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build())\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr_ti, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator_ti, \\\n",
    "                    numFolds=5)\n",
    "assembler_ti_cv = VectorAssembler(\n",
    "    inputCols=['rawFeatures_name','rawFeatures_title','rawFeatures_new','rawFeatures_old','rawFeatures_comment','features_idf_name','features_idf_title','features_idf_new','features_idf_old','features_idf_comment'],\n",
    "    outputCol='features')\n",
    "assembler_ti_cv.transform(trainingData_ti)\n",
    "pipeline_ti_cv = Pipeline(stages=[assembler_ti_cv, cv])\n",
    "trainingData_ti.cache()\n",
    "testData_ti.cache()\n",
    "model_ti_cv = pipeline_ti_cv.fit(trainingData_ti)\n",
    "\n",
    "predictions = model_ti_cv.transform(testData_ti)\n",
    "# Evaluate best model\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
