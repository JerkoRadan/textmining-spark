{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all required libraries here. There are some stuff that I had used previously but have not removed here\n",
    "from pyspark import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import os\n",
    "from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType\n",
    "#importing pyspark machine learning libraries\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have a 8 gb ram so I have 7.6 gb available to use so I am allowing max java heap to use 6 gb\n",
    "from pyspark import SparkConf\n",
    "conf=SparkConf()\n",
    "conf.set(\"spark.driver.memory\", \"6g\")\n",
    "conf.set(\"spark.executor.memory\", \"5g\")\n",
    "conf.set(\"spark.driver.cores\", \"4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining spark context and spark session\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the schema of the json texts to be imported\n",
    "schema = StructType([\n",
    "    StructField('title_page', StringType(), True),\n",
    "    StructField('text_new', StringType(), True),\n",
    "    StructField('text_old', StringType(), True),\n",
    "    StructField('name_user', StringType(), True),\n",
    "    StructField('label', StringType(), True),\n",
    "    StructField('comment', StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this would recursively read all json files into the same data frame. ** wildcard refers to all directories\n",
    "#1.6 gb of data has been used\n",
    "#This is the easiest way to read json in spark\n",
    "df_json = spark.read.json(\"/home/shourya/adv_assignment_3/**/part-*\",schema, multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting unique tokens in labels\n",
    "from pyspark.sql.functions import col\n",
    "df_json.groupBy(\"label\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the name of the column 'label' to 'category' as pyspark logistic model does not accept anything but 'label' as target.\n",
    "df_json = df_json.withColumnRenamed(\"label\", \"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating class weights here\n",
    "import numpy as np\n",
    "y_collect = df_json.select(\"category\").groupBy(\"category\").count().collect()\n",
    "unique_y = [x[\"category\"] for x in y_collect]\n",
    "total_y = sum([x[\"count\"] for x in y_collect])\n",
    "unique_y_count = len(y_collect)\n",
    "bin_count = [x[\"count\"] for x in y_collect]\n",
    "\n",
    "class_weights_spark = {i: ii for i, ii in zip(unique_y, total_y / (unique_y_count * np.array(bin_count)))}\n",
    "print(class_weights_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping to categories\n",
    "from pyspark.sql import functions as F\n",
    "from itertools import chain\n",
    "\n",
    "mapping_expr = F.create_map([F.lit(x) for x in chain(*class_weights_spark.items())])\n",
    "\n",
    "df_json = df_json.withColumn(\"weight\", mapping_expr.getItem(F.col(\"category\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "# regular expression tokenizer\n",
    "regexTokenizer_new = RegexTokenizer(inputCol=\"text_new\", outputCol=\"words_new\", pattern=\"\\\\W\")\n",
    "regexTokenizer_old = RegexTokenizer(inputCol=\"text_old\", outputCol=\"words_old\", pattern=\"\\\\W\")\n",
    "regexTokenizer_title = RegexTokenizer(inputCol=\"title_page\", outputCol=\"words_title_page\", pattern=\"\\\\W\")\n",
    "regexTokenizer_user = RegexTokenizer(inputCol=\"name_user\", outputCol=\"words_name_user\", pattern=\"\\\\W\")\n",
    "regexTokenizer_comment = RegexTokenizer(inputCol=\"comment\", outputCol=\"words_comment\", pattern=\"\\\\W\")\n",
    "# stop words\n",
    "add_stopwords = [\"a\",\"an\",\"the\",\"on\",]\n",
    "stopwordsRemover_new = StopWordsRemover(inputCol=\"words_new\", outputCol=\"filtered_new\").setStopWords(add_stopwords)\n",
    "stopwordsRemover_old = StopWordsRemover(inputCol=\"words_old\", outputCol=\"filtered_old\").setStopWords(add_stopwords)\n",
    "stopwordsRemover_comment = StopWordsRemover(inputCol=\"words_comment\", outputCol=\"filtered_comment\").setStopWords(add_stopwords)\n",
    "# bag of words count\n",
    "countVectors_new = CountVectorizer(inputCol=\"filtered_new\", outputCol=\"features_new\", vocabSize=100000, minDF=5)\n",
    "countVectors_old = CountVectorizer(inputCol=\"filtered_old\", outputCol=\"features_old\", vocabSize=100000, minDF=5)\n",
    "countVectors_comment = CountVectorizer(inputCol=\"filtered_comment\", outputCol=\"features_comment\", vocabSize=100000, minDF=5)\n",
    "countVectors_user = CountVectorizer(inputCol=\"words_name_user\", outputCol=\"features_name_user\", vocabSize=100000, minDF=5)\n",
    "countVectors_title = CountVectorizer(inputCol=\"words_title_page\", outputCol=\"features_title_page\", vocabSize=100000, minDF=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "#This would encode label into numerical values\n",
    "label_stringIdx = StringIndexer(inputCol = \"category\", outputCol = \"label\")\n",
    "\n",
    "#Pipeline for creating jobs by spark. This does not have a logical consequence in our case but essential.\n",
    "pipeline = Pipeline(stages=[label_stringIdx, regexTokenizer_new, regexTokenizer_old, regexTokenizer_title, regexTokenizer_user, regexTokenizer_comment, stopwordsRemover_new, stopwordsRemover_old, stopwordsRemover_comment, countVectors_comment, countVectors_new, countVectors_old, countVectors_user, countVectors_title])\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(df_json)\n",
    "dataset = pipelineFit.transform(df_json)\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "#Training and test split\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression model definition. Classweights have been added\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0,weightCol=\"weight\")\n",
    "#Pyspark would have us assemble the features using vector assembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['features_new','features_old','features_comment','features_name_user','features_title_page'],\n",
    "    outputCol='features')\n",
    "#Transforming the training data using the assembler\n",
    "assembler.transform(trainingData)\n",
    "#Pipeline based execution which is charcteristic for spark.\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "#Fitting the model\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the model performance on testData\n",
    "predictions = model.transform(testData)\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)\n",
    "#without classweights result is 0.8143742529986768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "#TF IDF based transformation in spark\n",
    "hashingTF_name = HashingTF(inputCol=\"words_name_user\", outputCol=\"rawFeatures_name\", numFeatures=100)\n",
    "hashingTF_title = HashingTF(inputCol=\"words_title_page\", outputCol=\"rawFeatures_title\", numFeatures=100)\n",
    "hashingTF_new = HashingTF(inputCol=\"filtered_new\", outputCol=\"rawFeatures_new\", numFeatures=100)\n",
    "hashingTF_old = HashingTF(inputCol=\"filtered_old\", outputCol=\"rawFeatures_old\", numFeatures=100)\n",
    "hashingTF_comment = HashingTF(inputCol=\"filtered_comment\", outputCol=\"rawFeatures_comment\", numFeatures=100)\n",
    "idf_name = IDF(inputCol=\"rawFeatures_name\", outputCol=\"features_idf_name\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "idf_title = IDF(inputCol=\"rawFeatures_title\", outputCol=\"features_idf_title\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "idf_new = IDF(inputCol=\"rawFeatures_new\", outputCol=\"features_idf_new\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "idf_old = IDF(inputCol=\"rawFeatures_old\", outputCol=\"features_idf_old\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "idf_comment = IDF(inputCol=\"rawFeatures_comment\", outputCol=\"features_idf_comment\", minDocFreq=5) #minDocFreq: remove sparse terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same style of execution as the first logistic model now using TF IDF based features along with previous features\n",
    "pipeline_ti = Pipeline(stages=[label_stringIdx, regexTokenizer_new, regexTokenizer_old, regexTokenizer_title, regexTokenizer_user, regexTokenizer_comment, stopwordsRemover_new, stopwordsRemover_old, stopwordsRemover_comment, countVectors_comment, countVectors_new, countVectors_old, countVectors_user, countVectors_title, hashingTF_name, hashingTF_title, hashingTF_new, hashingTF_old, hashingTF_comment, idf_name, idf_title, idf_new, idf_old, idf_comment])\n",
    "pipelineFit_ti = pipeline_ti.fit(df_json)\n",
    "dataset_ti = pipelineFit_ti.transform(df_json)\n",
    "(trainingData_ti, testData_ti) = dataset_ti.randomSplit([0.7, 0.3], seed = 100)\n",
    "lr_ti = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0,weightCol=\"weight\")\n",
    "assembler_ti = VectorAssembler(\n",
    "    inputCols=['rawFeatures_name','rawFeatures_title','rawFeatures_new','rawFeatures_old','rawFeatures_comment','features_idf_name','features_idf_title','features_idf_new','features_idf_old','features_idf_comment'],\n",
    "    outputCol='features')\n",
    "assembler_ti.transform(trainingData_ti)\n",
    "pipeline_ti = Pipeline(stages=[assembler_ti, lr_ti])\n",
    "trainingData_ti.cache()\n",
    "testData_ti.cache()\n",
    "model_ti = pipeline_ti.fit(trainingData_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model without cross validation\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator_ti = MulticlassClassificationEvaluator(predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 fold cross validation with tf idf features and class weights\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "#In this section we try to run the previous model with various parameter values\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr_ti.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr_ti.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "#            .addGrid(model_ti.maxIter, [10, 20, 50]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build())\n",
    "# Create 10-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr_ti, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator_ti, \\\n",
    "                    numFolds=10)\n",
    "assembler_ti_cv = VectorAssembler(\n",
    "    inputCols=['rawFeatures_name','rawFeatures_title','rawFeatures_new','rawFeatures_old','rawFeatures_comment','features_idf_name','features_idf_title','features_idf_new','features_idf_old','features_idf_comment'],\n",
    "    outputCol='features')\n",
    "assembler_ti_cv.transform(trainingData_ti)\n",
    "pipeline_ti_cv = Pipeline(stages=[assembler_ti_cv, cv])\n",
    "trainingData_ti.cache()\n",
    "testData_ti.cache()\n",
    "model_ti_cv = pipeline_ti_cv.fit(trainingData_ti)\n",
    "\n",
    "prediction = model_ti_cv.transform(testData_ti)\n",
    "# Evaluate best model\n",
    "evaluator_ti = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator_ti.evaluate(prediction)\n",
    "#performance with class weights is 0.8846150805585391"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of naive bayes with only primary features. Performance is 0.76960714706985\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "nb = NaiveBayes(smoothing=1,weightCol=\"weight\")\n",
    "#Pyspark would have us assemble the features using vector assembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['features_new','features_old','features_comment','features_name_user','features_title_page'],\n",
    "    outputCol='features')\n",
    "#Transforming the training data using the assembler\n",
    "assembler.transform(trainingData)\n",
    "#Pipeline based execution which is charcteristic for spark.\n",
    "pipeline = Pipeline(stages=[assembler, nb])\n",
    "#Fitting the model\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "prediction = model.transform(testData)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest implmentation without proper sampling. Performance is at 0.76\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32\n",
    "                           )\n",
    "#Pyspark would have us assemble the features using vector assembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['features_new','features_old','features_comment','features_name_user','features_title_page'],\n",
    "    outputCol='features')\n",
    "#Transforming the training data using the assembler\n",
    "assembler.transform(trainingData)\n",
    "#Pipeline based execution which is charcteristic for spark.\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "#Fitting the model\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "prediction = model.transform(testData)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation of smote\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "X = dataset_ti.toPandas().filter(items=[\"rawFeatures_name\", \"rawFeatures_title\", \"rawFeatures_new\",\"hypertension\",\"rawFeatures_old\",\"rawFeatures_comment\",\"features_idf_name\",\"features_idf_title\",\"features_idf_new\",\"features_idf_old\",\"features_idf_comment\"])\n",
    "Y = dataset_ti.toPandas().filter(items=[\"label\"])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
    "sm = SMOTE(random_state=12) # ratio = \"auto\" by default, kind = \"regular\" by default\n",
    "x_train_res, y_train_res = sm.fit_sample(X_train, Y_train)\n",
    "print(‘Resampled dataset shape {}’.format(Counter(y_train_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_1 = pd.DataFrame(x_train_res,columns=[\"rawFeatures_name\", \"rawFeatures_title\", \"rawFeatures_new\",\"hypertension\",\"rawFeatures_old\",\"rawFeatures_comment\",\"features_idf_name\",\"features_idf_title\",\"features_idf_new\",\"features_idf_old\",\"features_idf_comment\"])\n",
    "\n",
    "dataframe_2 = pd.DataFrame(y_train_res, columns = [\"label\"])\n",
    "# frames = [dataframe_1, dataframe_2]\n",
    "result = dataframe_1.combine_first(dataframe_2)\n",
    "imputeDF_1 = spark.createDataFrame(result)\n",
    "(trainingData, testData) = imputeDF_1.randomSplit([0.7, 0.3], seed = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest on class balanced data\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32\n",
    "                           )\n",
    "#Pyspark would have us assemble the features using vector assembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"rawFeatures_name\", \"rawFeatures_title\", \"rawFeatures_new\",\"hypertension\",\"rawFeatures_old\",\"rawFeatures_comment\",\"features_idf_name\",\"features_idf_title\",\"features_idf_new\",\"features_idf_old\",\"features_idf_comment\"],\n",
    "    outputCol=\"features\")\n",
    "#Transforming the training data using the assembler\n",
    "assembler.transform(trainingData)\n",
    "#Pipeline based execution which is charcteristic for spark.\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "#Fitting the model\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "prediction = model.transform(testData)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
